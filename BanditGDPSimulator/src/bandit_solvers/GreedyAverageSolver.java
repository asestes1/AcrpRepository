package bandit_solvers;

import java.util.HashMap;
import java.util.HashSet;
import java.util.Iterator;
import java.util.List;
import java.util.Map;
import java.util.Set;
import java.util.function.BiFunction;

import org.apache.commons.math3.linear.RealVector;

import bandit_objects.SimpleTmiAction;

/**
 * This is an algorithm for solving similarity-context bandit problems that estimates
 * the value of each TMIAction by taking a weighted average of the results of 
 * TMIActions. The weighting is generated by comparing both the TMIAction and the
 * context. The highest estimate is then chosen. 
 * @author Alex
 *
 */
public class GreedyAverageSolver extends IndexSolver {
	private final BiFunction<SimpleTmiAction,SimpleTmiAction,Double> tmiComparer;
	
	public GreedyAverageSolver(BiFunction<SimpleTmiAction,SimpleTmiAction,Double> tmi_comparer) {
		super();
		this.tmiComparer = tmi_comparer;
	}
	
	@Override
	public void reset() {
		super.reset();
	}
	
	@Override
	public Map<SimpleTmiAction,Double> getIndices(RealVector context_sims, int remaining_time) {
		Map<SimpleTmiAction,Double> estimates = new HashMap<SimpleTmiAction,Double>();
		
		int historyLength = actionHistory.size();
		Set<SimpleTmiAction> setTmis = new HashSet<SimpleTmiAction>(actionHistory);
		int numUniqueTmis = setTmis.size();
		Iterator<SimpleTmiAction> choiceIter = setTmis.iterator();
		for(int i=0; i< numUniqueTmis;i++){
			SimpleTmiAction nextAction = choiceIter.next();
			
			//Get the reward that happened for the state we are currently examining.
			Iterator<Double> rewardIter = rewardHistory.listIterator();
			Iterator<SimpleTmiAction> compareChoiceIter = actionHistory.listIterator();
			Iterator<List<Double>> contextIter = contextHistory.listIterator();
			//Compare each other action with the current action. This gives
			//a similarity weight. Weight the reward by this weight and add 
			//it to the sum.
			double weightedSum = 0.0;
			double totalWeight =0.0;
			for(int j=0;j < historyLength;j++){
				SimpleTmiAction otherAction = compareChoiceIter.next();
				double actionSimilarity = 
						tmiComparer.apply(nextAction,otherAction);
				double contextSimilarity = contextIter.next().get(i);
				double reward = rewardIter.next();
				weightedSum += actionSimilarity*contextSimilarity*reward;
				totalWeight += actionSimilarity*contextSimilarity;
			}
			estimates.put(nextAction, weightedSum/totalWeight);
		}
		return estimates;
	}

}
